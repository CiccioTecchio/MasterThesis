Gli Online Social Network(OSN) sono tra le applicazioni che hanno maggiore utenza e posseggono gran parte dei dati personali dei loro utenti.\newline
La gestione dei dati personali e sensibili degli utenti da parte di questi colossi dell'IT viene regolata attraverso apposite leggi, come ad esempio il GDPR in Europa, ma spesso sono gli utenti stessi a pubblicare involontariamente delle informazioni sensibili che possono esporli a dei rischi. In particolare, gli studi dimostrano che gli utenti a volte rivelano troppe informazioni o rilasciano involontariamente messaggi di rimpianto, soprattutto quando sono negligenti, emotivi o ignari dei rischi per la propria privacy. Ad esempio, si è spesso sentito parlare di furti in appartamento avvenuti proprio perché i ladri, attraverso gli OSN, erano venuti a conoscenza del periodo di vacanza dei soggetti da derubare. Pertanto, esistono grandi esigenze nel poter identificare contenuti online potenzialmente sensibili, in modo che gli utenti possano essere avvisati prima che divulghino tali dati.

A questo scopo è stato sviluppato un plugin per Google Chrome, chiamato \textit{Knoxly}, che attraverso l'utilizzo di regular expression e dizionari segnala all'utente la presenza di dati sensibili e/o personali all'interno di un messaggio scritto in testo naturale.

L'obiettivo di questa tesi è estendere le funzionalità di \textit{Knoxly}, per andare oltre i semplici meccanismi di string-matching. In particolare si vuole aggiungere la possibilità di riconoscere dati sensibili in base al contesto in cui sono stati scritti e in base a quanto l'utente reputi sensibili questi dati. Per raggiungere questo obiettivo è stato sviluppato un modello basato su Intelligenza Artificiale che comprende quattro moduli principali: (a) \textit{embedding module}, (b) \textit{topic classifier}, (c) \textit{sensitiveness classifier} e (d) \textit{customized sensitiveness classifier}. L'\textit{embedding module} è basato su tecniche di sentence embedding, il suo compito è di trasformare il testo in linguaggio naturale in un vettore n-dimensionale di lunghezza fissa. Il \textit{topic classifier} è basato sul modello Random Forest, il suo compito è comprendere l'argomento di cui parla il testo. Il \textit{sensitiveness classifier} è basato sul modello Random Forest, il suo compito è di rilevare il livello di sensibilità dei contenuti del testo in base all'argomento. Infine, il \textit{customized sensitiveness classifier} è basato sul modello Passive-Aggressive (in particolare PAII), il suo compito è comprendere la propensione dell'utente alla divulgazione di determinati contenuti online ed imparare dai suoi feedback in modo da non segnalare contenuti che per l'utente sono innocui. 